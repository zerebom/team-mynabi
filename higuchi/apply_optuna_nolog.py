#!/usr/bin/env python
# coding: utf-8

# In[1]:

import datetime
import optuna
from functools import partial
import numpy as np
import pandas as pd
import scipy as sp
from scipy import stats
from scipy.stats import norm, skew
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
import matplotlib
font = {'family': 'Yu Mincho'}
matplotlib.rc('font', **font)

pd.set_option('max_columns', 1000)
pd.set_option('max_rows', 1000)

import warnings
warnings.filterwarnings('ignore')

import re
import geocoder
from geopy.distance import great_circle, vincenty
from tqdm import tqdm
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
import os
import gc
import lightgbm as lgb
from sklearn.model_selection import KFold, train_test_split
from time import time
import datetime
from script import RegressionPredictor
import japanize_matplotlib

SEED = 1234
n_splits = 5


'''
16983.25780563056(no_logs)
'reg_lambda': 0.3684160201142433, 'reg_alpha': 0.26283703088851534, 'colsample_bytree': 0.5, 'subsample': 0.8, 'max_depth': 8, 'min_child_weight': 20

19172.01030796285 
 {'reg_lambda': 13.569568709038572, 'reg_alpha': 0.03834093298774701, 'colsample_bytree': 0.6, 'subsample': 0.5, 'max_depth': 8, 'min_child_weight': 31}
 17562.816975160968
  with parameters: {'reg_lambda': 42.55553901354264, 'reg_alpha': 0.0034913931092345903, 'colsample_bytree': 0.8, 'subsample': 0.8, 'max_depth': 8, 'min_child_weight': 13}
'''

train = pd.read_csv('./input/prep_train1030.csv')
test = pd.read_csv('./input/prep_train1030.csv')
y_train = train['賃料']


# In[3]:


drop_col = ['id', '賃料']
## 必要な特徴量に絞る
y_train = train['賃料']
y_train_log = np.log1p(y_train)
X_train = train.drop(drop_col, axis=1)
X_test = test.drop(drop_col, axis=1)

train_idx = len(X_train)
data = pd.concat([X_train, X_test])
many_catego_cols = ['city', 'city2', 'nearest_sta', 'second_sta', 'third_sta']
tmp_data = data[many_catego_cols]
data = pd.get_dummies(data.drop(columns=many_catego_cols), drop_first=True)
data = pd.concat([data, tmp_data], axis=1)
X_train = data[:train_idx]
X_test = data[train_idx:]
features = ['面積', '築年数', 'sta_min', 'center_dis', 'loc_lat', 'loc_lon', '総階数', '畳', '所在階', '追焚機能', '方角_南', 'BSアンテナ', 'L', '建物構造_RC（鉄筋コンクリート）', '戸建て',
            '室内洗濯機置場', 'IHコンロ', '駐輪場_無', '駐車場_空有', 'ウォークインクローゼット', '部屋数']
X_train = X_train[features]
X_test = X_test[features]


# In[4]:

#ここに学習したいハイパーパラメータを入れる
#学習したいやつはtrial.suggest_hoge()メソッドに渡す。
def get_default_parameter_suggestions(trial):
    """
    Get parameter sample for Boosting (like XGBoost, LightGBM)

    Args:
        trial(trial.Trial):

    Returns:
        dict: parameter sample generated by trial object
    """
    return {
        'num_iterations': 50000,
        'boosting_type': 'gbdt',
        'objective': 'regression',
        'metric': 'rmse',
        'random_state': 0,
        'verbose': -1,
        'random_state': 0,
        'early_stopping_round': 100,
        # L2 正則化
        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 1e3),
        # L1 正則化
        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 1e3),
        # 弱学習木ごとに使う特徴量の割合
        # 0.5 だと全体のうち半分の特徴量を最初に選んで, その範囲内で木を成長させる
        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.5, 1.0, .1),
        # 学習データ全体のうち使用する割合
        # colsample とは反対に row 方向にサンプルする
        'subsample': trial.suggest_discrete_uniform('subsample', .5, 1., .1),
        # 木の最大の深さ
        # たとえば 5 の時各弱学習木の各データに対するルールは、最大でも5に制限される.
        'max_depth': trial.suggest_int('max_depth', 3, 8),
        # 末端ノードに含まれる最小のサンプル数
        # これを下回るような分割は作れなくなるため, 大きく設定するとより全体の傾向でしか分割ができなくなる
        # [NOTE]: 数であるのでデータセットの大きさ依存であることに注意
        'min_child_weight': trial.suggest_int('min_child_weight', 5, 40)
    }


# In[7]:


def objective(X_train, y_train, X_test, trial):
    params = get_default_parameter_suggestions(trial)
    kf = KFold(n_splits=n_splits, random_state=SEED)

    rmses = list()

    oof = pd.DataFrame({'id': list(train.id.values), 'y_train': list(y_train.values)})

    for fold, (trn_idx, val_idx) in enumerate(kf.split(X_train, y_train)):
        start_time = time()
        print('Training on fold {}'.format(fold + 1))

        tr_x, tr_y = X_train.iloc[trn_idx], y_train.iloc[trn_idx]
        vl_x, vl_y = X_train.iloc[val_idx], y_train.iloc[val_idx]

        tr_data = lgb.Dataset(tr_x, label=tr_y)
        vl_data = lgb.Dataset(vl_x, label=vl_y)
        clf = lgb.train(params, tr_data, 5000, valid_sets=[tr_data, vl_data], verbose_eval=5000)
        oof.loc[val_idx, 'oof'] = clf.predict(vl_x, num_iteration=clf.best_iteration)

        ## アンサンブル

        rmses.append(clf.best_score['valid_1']['rmse'])

        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data
        gc.collect()
    #return は最小化したいもの
    return np.sqrt(mean_squared_error(oof['oof'], oof['y_train']))


# In[ ]:

#parital...外側から引数を渡すために必要なメソッド
f = partial(objective, X_train, y_train, X_test)

#ハイパラ学習
study = optuna.create_study()
study.optimize(f, n_trials=300)

#bestパラメータの保存
with open(f'./best_params_by_oputuna{datetime.datetime.today()}.txt') as f:
    f.write(str(study.best_params()))

#学習の過程を保存
df = study.trials_dataframe()
df.to_csv(f'../optuna_study_log_{datetime.datetime.today()}.csv')
