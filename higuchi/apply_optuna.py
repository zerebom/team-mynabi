 #!/usr/bin/env python
# coding: utf-8

# In[1]:

import datetime
import optuna
from functools import partial
import numpy as np
import pandas as pd
import scipy as sp
from scipy import stats
from scipy.stats import norm, skew
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
import matplotlib
font = {'family': 'Yu Mincho'}
matplotlib.rc('font', **font)

pd.set_option('max_columns',1000)
pd.set_option('max_rows',1000)

import warnings
warnings.filterwarnings('ignore') 

import re
import geocoder
from geopy.distance import great_circle, vincenty
from tqdm import tqdm
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
import os
import gc
import lightgbm as lgb
from sklearn.model_selection import KFold, train_test_split
from time import time
import datetime
from script import RegressionPredictor
import japanize_matplotlib

SEED=1234
n_splits=5

import numpy as np
import pandas as pd
import scipy as sp
from scipy import stats
from scipy.stats import norm, skew
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
import matplotlib

pd.set_option('max_columns', 1000)
pd.set_option('max_rows', 1000)

import warnings
warnings.filterwarnings('ignore')

import re
import geocoder
from geopy.distance import great_circle, vincenty
from tqdm import tqdm
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
import os
import gc
import lightgbm as lgb
from sklearn.model_selection import KFold, train_test_split
from time import time
import datetime
from script import RegressionPredictor, LogRegressionPredictor, plot_scatter
import japanize_matplotlib
# print(os.listdir("././input"))
# print(os.listdir("././submit"))
from utils import save_data
SEED = 1234
n_splits = 10
import glob

X_train = pd.read_csv('./psuedo/X_train1107.csv')
train_idx=X_train['id']
X_test = pd.read_csv('./psuedo/X_test1107.csv')
y_train = X_train['賃料']
drop_col = ['id', '賃料']
y_train = X_train['賃料']

y_train_log = np.log1p(y_train)
X_train = X_train.drop(drop_col, axis=1)
X_test = X_test.drop('id', axis=1)
# In[4]:

#ここに学習したいハイパーパラメータを入れる
#学習したいやつはtrial.suggest_hoge()メソッドに渡す。
def get_default_parameter_suggestions(trial):
    """
    Get parameter sample for Boosting (like XGBoost, LightGBM)

    Args:
        trial(trial.Trial):

    Returns:
        dict: parameter sample generated by trial object
    """
    return {
        'num_iterations': 50000,
        'boosting_type': 'gbdt',
        'objective': 'regression',
        'metric': 'mae',
        'learning_rate': 0.05,
        'random_state': 0,
        'verbose': -1,
        'random_state': 0,
        'early_stopping_round': 100,
        # L2 正則化
        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 1e3),
        # L1 正則化
        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 1e3),
        # 弱学習木ごとに使う特徴量の割合
        # 0.5 だと全体のうち半分の特徴量を最初に選んで, その範囲内で木を成長させる
        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.5, 1.0, .1),
        # 学習データ全体のうち使用する割合
        # colsample とは反対に row 方向にサンプルする
        'subsample': trial.suggest_discrete_uniform('subsample', .5, 1., .1),
        # 木の最大の深さ
        # たとえば 5 の時各弱学習木の各データに対するルールは、最大でも5に制限される.
        'max_depth': trial.suggest_int('max_depth', 3, 8),
        # 末端ノードに含まれる最小のサンプル数
        # これを下回るような分割は作れなくなるため, 大きく設定するとより全体の傾向でしか分割ができなくなる
        # [NOTE]: 数であるのでデータセットの大きさ依存であることに注意
        'min_child_weight': trial.suggest_int('min_child_weight', 5, 40)
    }

    


# In[7]:


def objective(X_train,y_train,X_test,trial):
    params=get_default_parameter_suggestions(trial)
    kf = KFold(n_splits=n_splits, random_state=SEED)

    rmses = list()

    oof = pd.DataFrame({'id': list(train_idx.values), 'y_train': list(y_train.values)})

    for fold, (trn_idx, val_idx) in enumerate(kf.split(X_train, y_train_log)):
        start_time = time()
        print('Training on fold {}'.format(fold + 1))

        tr_x, tr_y = X_train.iloc[trn_idx], y_train_log.iloc[trn_idx]
        vl_x, vl_y = X_train.iloc[val_idx], y_train_log.iloc[val_idx]

        tr_data = lgb.Dataset(tr_x, label=tr_y)
        vl_data = lgb.Dataset(vl_x, label=vl_y)
        clf = lgb.train(params, tr_data, 50000, valid_sets = [tr_data, vl_data], verbose_eval=5000)
        oof.loc[val_idx,'oof'] = np.expm1(clf.predict(vl_x, num_iteration=clf.best_iteration))

        ## アンサンブル

        rmses.append(clf.best_score['valid_1']['l1'])

        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data
        gc.collect()
    #return は最小化したいもの
    return np.sqrt(mean_squared_error(oof['oof'], oof['y_train']))


# In[ ]:

#parital...外側から引数を渡すために必要なメソッド
f = partial(objective,X_train,y_train,X_test)

#ハイパラ学習
study = optuna.create_study()
study.optimize(f, n_trials=300)

#bestパラメータの保存
with open(f'./best_params_by_oputuna.txt') as f:
    f.write(str(study.best_params()))

#学習の過程を保存
df=study.trials_dataframe()
df.to_csv(f'../optuna_study_log.csv')


